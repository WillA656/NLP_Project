{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WillA656/NLP_Project/blob/main/SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ46j-PD-eXS"
      },
      "source": [
        "Let's set up SparkNLP.\n",
        "\n",
        "## Hello world -- Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz8walkjXq0I",
        "outputId": "539ac370-dc4d-45a7-c45a-400b1d00e0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 23:40:38--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 3.86.22.73\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|3.86.22.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-07-24 23:40:38--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "\r-                     0%[                    ]       0  --.-KB/s               \r-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-24 23:40:38 (27.2 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.4.1\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.4.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.2/579.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Data\n",
        "%%capture\n",
        "!curl \"https://raw.githubusercontent.com/WillA656/NLP_Project/main/Gutendex_JSON\" -o book_list\n",
        "!wget -i book_list\n",
        "\n",
        "# ! rm *.utf-8.* # remove all utf"
      ],
      "metadata": {
        "id": "6dnB-dtqsWU0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename files\n",
        "import os\n",
        "os.mkdir(\"book_data\")\n",
        "for filename in os.listdir(\".\"):\n",
        "  if filename.endswith(\"utf-8\"):\n",
        "    with open(filename) as f:\n",
        "      first_line = f.readline()\n",
        "      print(first_line[32:])\n",
        "    os.rename(filename, (f\"book_data/{first_line[32:]}\").rstrip()+\".txt\") # rename and move to new file"
      ],
      "metadata": {
        "id": "e3u9Wl4svEI2",
        "outputId": "d45d34d0-a047-4ae3-b4a1-e20a1357b045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Room with a View\n",
            "\n",
            "Romeo and Juliet\n",
            "\n",
            "Great Expectations\n",
            "\n",
            "Middlemarch\n",
            "\n",
            "The Blue Castle: a novel\n",
            "\n",
            "Little Women; Or, Meg, Jo, Beth, and Amy\n",
            "\n",
            "The Enchanted April\n",
            "\n",
            "Pride and Prejudice\n",
            "\n",
            "The Adventures of Ferdinand Count Fathom — Complete\n",
            "\n",
            "Moby Dick; Or, The Whale\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all 10 books into our corpus (Requires titles to end in .txt)\n",
        "output_path = '/content/corpus.txt'\n",
        "with open(output_path, 'w') as out:\n",
        "    for filename in os.listdir('book_data'):\n",
        "        file_path = os.path.join('book_data', filename)\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(file_path, 'r') as infile:\n",
        "                out.write(infile.read() + '\\n')"
      ],
      "metadata": {
        "id": "v3G9s1ld8fHG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8trkawwfX8bx",
        "outputId": "166f8c90-3d43-468b-a63a-4cba51baac96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.2.3)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.5)\n",
            "Collecting sparknlp\n",
            "  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.10/dist-packages (from sparknlp) (5.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sparknlp) (1.25.2)\n",
            "Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
            "Installing collected packages: sparknlp\n",
            "Successfully installed sparknlp-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install sparknlp\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSv2iBclzWMr",
        "outputId": "2152328f-9bef-4951-998b-937b4e65e777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "explain_document_ml download started this may take some time.\n",
            "Approx size to download 9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "pipeline = PretrainedPipeline(\"explain_document_ml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr, explode, arrays_zip\n",
        "df = spark.read.text(\"/content/corpus.txt\")\n",
        "df = df.withColumnRenamed(\"value\", \"text\")\n",
        "result = pipeline.transform(df)\n",
        "result = result.select(\n",
        "    col(\"token.result\").alias(\"token_result\"),\n",
        "    col(\"pos.result\").alias(\"pos_result\")\n",
        ")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIuDrRJeFdNK",
        "outputId": "af21b9a2-5cba-48ee-cc20-1b69c2917fe4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|        token_result|          pos_result|\n",
            "+--------------------+--------------------+\n",
            "|[The, Project, Gu...|[DT, NNP, NNP, NN...|\n",
            "|                  []|                  []|\n",
            "|[This, ebook, is,...|[DT, NN, VBZ, IN,...|\n",
            "|[most, other, par...|[JJS, JJ, NNS, IN...|\n",
            "|[whatsoever, ., Y...|[RB, ., PRP, MD, ...|\n",
            "|[of, the, Project...|[IN, DT, NNP, NNP...|\n",
            "|[at, www.gutenber...|[IN, NN, ., IN, P...|\n",
            "|[you, will, have,...|[PRP, MD, VB, TO,...|\n",
            "|[before, using, t...|[IN, VBG, DT, NN, .]|\n",
            "|                  []|                  []|\n",
            "|[Title, :, Great,...|  [NNP, :, NNP, NNP]|\n",
            "|                  []|                  []|\n",
            "|[Author, :, Charl...|   [NN, :, NNP, NNP]|\n",
            "|                  []|                  []|\n",
            "|[Release, date, :...|[NNP, NN, :, NNP,...|\n",
            "|[Most, recently, ...|[JJS, RB, VBN, :,...|\n",
            "|                  []|                  []|\n",
            "|[Language, :, Eng...|       [NNP, :, NNP]|\n",
            "|                  []|                  []|\n",
            "|[Credits, :, An, ...|[NNS, :, DT, NNP,...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvSSJOQ-kKm"
      },
      "source": [
        "We can use some recent headlines."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.printSchema()\n",
        "\n",
        "tokens_and_pos = result.select(explode(expr(\"arrays_zip(token_result, pos_result)\")).alias(\"zipped\"))\n",
        "\n",
        "tokens_and_pos_split = tokens_and_pos.selectExpr(\n",
        "    \"zipped.token_result as token\",\n",
        "    \"zipped.pos_result as pos\"\n",
        ")\n",
        "tokens_and_pos_split.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KorQJT3HImxC",
        "outputId": "d339f970-e0f4-450d-e9cb-353b3feb8d36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- token_result: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- pos_result: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+------------+----+\n",
            "|       token| pos|\n",
            "+------------+----+\n",
            "|         The|  DT|\n",
            "|     Project| NNP|\n",
            "|   Gutenberg| NNP|\n",
            "|       eBook|  NN|\n",
            "|          of|  IN|\n",
            "|       Great| NNP|\n",
            "|Expectations| NNP|\n",
            "|        This|  DT|\n",
            "|       ebook|  NN|\n",
            "|          is| VBZ|\n",
            "|         for|  IN|\n",
            "|         the|  DT|\n",
            "|         use|  NN|\n",
            "|          of|  IN|\n",
            "|      anyone|  NN|\n",
            "|    anywhere|  RB|\n",
            "|          in|  IN|\n",
            "|         the|  DT|\n",
            "|      United| NNP|\n",
            "|      States|NNPS|\n",
            "+------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pronoun_tags = [\"PRP\", \"PRP$\"]\n",
        "pronouns_df = tokens_and_pos_split.filter(col(\"pos\").isin(pronoun_tags))\n",
        "pronouns_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCiYqvXikVZh",
        "outputId": "5b49e991-c7d2-4844-a249-99d8f6037bbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "| token| pos|\n",
            "+------+----+\n",
            "|   You| PRP|\n",
            "|    it| PRP|\n",
            "|    it| PRP|\n",
            "|    it| PRP|\n",
            "|   you| PRP|\n",
            "|   you| PRP|\n",
            "|   you| PRP|\n",
            "|     I| PRP|\n",
            "|     I| PRP|\n",
            "|    My|PRP$|\n",
            "|    my|PRP$|\n",
            "|    my|PRP$|\n",
            "|     I| PRP|\n",
            "|myself| PRP|\n",
            "|     I| PRP|\n",
            "|    my|PRP$|\n",
            "|   his|PRP$|\n",
            "|    my|PRP$|\n",
            "|     I| PRP|\n",
            "|    my|PRP$|\n",
            "+------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gendered_pronouns = [\"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"they\", \"them\", \"their\", \"theirs\"]\n",
        "\n",
        "gendered_pronouns_df = pronouns_df.filter(col(\"token\").isin(gendered_pronouns))\n",
        "gendered_pronouns_df.show()\n",
        "\n",
        "len(gendered_pronouns_df.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzS525UBArT",
        "outputId": "77489644-d1eb-492d-cac7-a0e3dc25ba7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+\n",
            "|token| pos|\n",
            "+-----+----+\n",
            "|  his|PRP$|\n",
            "| them| PRP|\n",
            "|their|PRP$|\n",
            "| they| PRP|\n",
            "|their|PRP$|\n",
            "|   he| PRP|\n",
            "|their|PRP$|\n",
            "| they| PRP|\n",
            "|their|PRP$|\n",
            "|their|PRP$|\n",
            "|their|PRP$|\n",
            "| them| PRP|\n",
            "|  his|PRP$|\n",
            "|  his|PRP$|\n",
            "|  his|PRP$|\n",
            "|   he| PRP|\n",
            "| them| PRP|\n",
            "|   he| PRP|\n",
            "|   he| PRP|\n",
            "|   he| PRP|\n",
            "+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "contingency_df = gendered_pronouns_df.groupBy(\"token\", \"pos\").agg(F.count(\"*\").alias(\"count\"))\n",
        "\n",
        "contingency_df.show()"
      ],
      "metadata": {
        "id": "-eKujOjWEne5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for ease of use\n",
        "contingency_pd = contingency_df.toPandas()\n",
        "\n",
        "# Pivot the DataFrame to get a matrix form (tokens as rows, pos as columns)\n",
        "pivot_table = contingency_pd.pivot(index='token', columns='pos', values='count').fillna(0)\n",
        "\n",
        "# Perform the chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(pivot_table)\n",
        "\n",
        "# Display the results\n",
        "print(\"Chi-Squared Statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"Expected Frequencies:\\n\", expected)"
      ],
      "metadata": {
        "id": "VC-21qMEpLWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}